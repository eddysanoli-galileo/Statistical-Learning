{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "658dc12c475a3a8caebf03b24f414cffa2901ebd330ffd26b9c22f028a90850c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Proyecto Final - Deployment\n",
    "\n",
    "En el notebook presentado a continuación se presenta una simulación del deployment de los diferentes modelos creados en el Notebook principal denominado \"Proyecto Final.ipynb\"\n",
    "\n",
    "## Carga de Paquetes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Enabled compatitility to tf1.x\n"
     ]
    }
   ],
   "source": [
    "# Paquetes utilizados\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import collections\n",
    "from pandas.api.types import is_integer_dtype, is_float_dtype\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "\n",
    "# Habilitar la compatibilidad con tensorflow v1 si se tienen tensorflow v2\n",
    "if tf.__version__.startswith(\"2.\"):\n",
    "  import tensorflow.compat.v1 as tf\n",
    "  tf.compat.v1.disable_v2_behavior()\n",
    "  tf.compat.v1.disable_eager_execution()\n",
    "  print(\"Enabled compatitility to tf1.x\")"
   ]
  },
  {
   "source": [
    "## Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se guardan los datos del CSV en un dataframe de pandas\n",
    "raw_dataset = pd.read_csv(\"data_titanic_proyecto.csv\")\n",
    "\n",
    "# ================================\n",
    "# Separación de \"Name\" \n",
    "# ================================\n",
    "\n",
    "# Se crea un \"feature engineering\" dataset\n",
    "fe_dataset = raw_dataset.copy()\n",
    "\n",
    "# Se separa la columna de \"name\" en \"first_name\" y \"last_name\"\n",
    "# Se emplea la opción \"expand\" para crear dos columnas a partir de una\n",
    "fe_dataset[[\"last_name\", \"first_name\"]] = raw_dataset[\"Name\"].str.split(\", \", expand = True)\n",
    "\n",
    "# Se extrae el honorífico del \"first_name\" utilizando el punto como un separador\n",
    "fe_dataset[[\"honorific\", \"first_name\"]] = fe_dataset[\"first_name\"].str.split(\".\", expand = True)\n",
    "\n",
    "# Elimina la columna de \"name\"\n",
    "fe_dataset = fe_dataset.drop(\"Name\", axis = 1)\n",
    "\n",
    "# ================================\n",
    "# Creación de \"married\"\n",
    "# ================================\n",
    "\n",
    "# Se obtiene el número de fila de todas las \"mrs\"\n",
    "mrs = fe_dataset[\"honorific\"] == \"Mrs\"\n",
    "\n",
    "# Se extraen los nombres que contienen paréntesis\n",
    "parenthesis = fe_dataset[\"first_name\"].str.contains('(', regex = False)\n",
    "\n",
    "# Este comando tiene dos partes\n",
    "# Derecha: \n",
    "#   1. Se obtienen todos los \"first_names\" de aquellas \"mrs\" que contienen paréntesis. \n",
    "#   2. Se utiliza una expresión regular para separar el nombre de casada y soltera\n",
    "#   3. La expresión genera 3 columnas nuevas \"0\" (copia de la siguiente), \"maiden_name\" y \"single_name\"\n",
    "# Izquierda:\n",
    "#   1. Se agregan las 3 nuevas columnas creadas por el regex al dataframe\n",
    "#   2. Solo se agregan datos a las filas que son \"mrs\" con paréntesis en su nombre\n",
    "fe_dataset.loc[mrs & parenthesis, [\"temp\", \"married_name\", \"single_name\"]] = fe_dataset.loc[mrs & parenthesis, \"first_name\"].str.extract('((?P<married_name>.+?) )?\\((?P<single_name>.+)\\)', expand = True)\n",
    "\n",
    "# Se elimina la columna \"temp\" \n",
    "fe_dataset = fe_dataset.drop(\"temp\", axis = 1)\n",
    "\n",
    "# Se construye el nombre completo del esposo de cada Mrs\n",
    "full_spouse_name = fe_dataset[\"married_name\"].astype(str) + \" \" + fe_dataset[\"last_name\"]\n",
    "\n",
    "# Se eliminan las filas que contienen \"nan\"\n",
    "# O mejor dicho, se mantienen solo las filas que no tienen \"NaNs\"\n",
    "full_spouse_name = full_spouse_name[~full_spouse_name.str.contains(\"nan\")]\n",
    "\n",
    "# Se convierte el dataframe en una lista de strings\n",
    "full_spouse_name = list(full_spouse_name)\n",
    "\n",
    "# Se vuelven a construir los nombres completos \n",
    "full_name = fe_dataset[\"first_name\"] + \" \" + fe_dataset[\"last_name\"]\n",
    "\n",
    "# Se elimina el texto adicional entre paréntesis de los nombres completos\n",
    "full_name = full_name.str.replace(' \\(.*\\)', '', regex = True)\n",
    "\n",
    "# Se elimina el texto adicional entre comillas\n",
    "full_name = full_name.str.replace(' \".*\"', '', regex = True)\n",
    "\n",
    "# Se revisa si alguno de los nombres en \"full_name\" están en la\n",
    "# lista de \"full_spouse_name\"\n",
    "mr_married = full_name.isin(full_spouse_name)\n",
    "\n",
    "# Se crea una nueva columna denominada \"married\"\n",
    "# Inicialmente se asume que nadie está casado\n",
    "fe_dataset[\"married\"] = 0\n",
    "\n",
    "# Se coloca como \"casada\" la persona que es una \"mrs\" (el honorífico indica esto)\n",
    "# o que se comprobó que estaba casada con una \"mrs\"\n",
    "fe_dataset.loc[mr_married | mrs, \"married\"] = 1\n",
    "\n",
    "# Se eliminan las columnas sobrantes luego del proceso de determinar a los casados\n",
    "# \"first_name\", \"married_name\" y \"single_name\".\n",
    "fe_dataset = fe_dataset.drop([\"first_name\", \"married_name\", \"single_name\"], axis = 1)\n",
    "\n",
    "# ================================\n",
    "# Eliminación de columnas\n",
    "# ================================\n",
    "\n",
    "# Se elimina la columna de ID\n",
    "fe_dataset = fe_dataset.drop(\"PassengerId\", axis = 1)\n",
    "\n",
    "# Se elimina la columna de Cabin\n",
    "fe_dataset = fe_dataset.drop(\"Cabin\", axis = 1)\n",
    "\n",
    "# ================================\n",
    "# Imputación de NaNs \"Embarked\"\n",
    "# ================================\n",
    "\n",
    "# Se obtiene la moda de \"Embarked\" y se extrae su valor \n",
    "# Luego se reemplazan todos los NaNs (isna()) por la moda anterior\n",
    "fe_dataset.loc[fe_dataset[\"Embarked\"].isna(), \"Embarked\"] = fe_dataset[\"Embarked\"].mode().values[0]\n",
    "\n",
    "# ================================\n",
    "# Label encoding \"class\" y \"survived\"\n",
    "# ================================\n",
    "\n",
    "# Se crea el objeto para label encoding (LE) para cada columna\n",
    "LE_class = preprocessing.LabelEncoder()\n",
    "LE_survived = preprocessing.LabelEncoder()\n",
    "\n",
    "# Se hace el label encoding para cada variable\n",
    "fe_dataset[\"passenger_class_enc\"] = LE_class.fit_transform(fe_dataset[\"passenger_class\"])\n",
    "fe_dataset[\"passenger_survived_enc\"] = LE_survived.fit_transform(fe_dataset[\"passenger_survived\"])\n",
    "\n",
    "# ================================\n",
    "# OHE \"passenger_sex\"\n",
    "# ================================\n",
    "\n",
    "# Se crea el objeto para one hot encoding (OHE) de cada columna\n",
    "OHE_sex = preprocessing.OneHotEncoder()\n",
    "\n",
    "# Antes de aplicar el encoding, la data debe ser redimensionada usando \"array.reshape(-1,1)\"\n",
    "data_to_encode = fe_dataset[\"passenger_sex\"].values.reshape(-1,1)\n",
    "\n",
    "# Se codifica el array anterior y luego se convierte como tal a un array\n",
    "# (ya que la salida se obtiene como una \"sparse matrix\")\n",
    "data_matrix_enc = OHE_sex.fit_transform(data_to_encode).toarray()\n",
    "\n",
    "# 1. Se convierte la matriz anterior en un dataframe con ints\n",
    "# 2. Se agregan las columnas generadas al dataset\n",
    "fe_dataset[[\"Female\", \"Male\"]] = pd.DataFrame(data_matrix_enc, columns = [\"Female\", \"Male\"]).astype(int)\n",
    "\n",
    "# Para análisis posteriores, se crea una columna con \"one-hot-encoding k-1\" para\n",
    "# el sexo (básicamente lo mismo que la columna \"Female\").\n",
    "fe_dataset[\"passenger_sex_enc\"] = fe_dataset[\"passenger_sex\"].astype('category')\n",
    "fe_dataset[\"passenger_sex_enc\"] = fe_dataset[\"passenger_sex_enc\"].cat.codes\n",
    "\n",
    "# =======================\n",
    "# OHE \"embarked\"\n",
    "# =======================\n",
    "\n",
    "# Se crea el objeto para one hot encoding (OHE) de cada columna\n",
    "OHE_embarked = preprocessing.OneHotEncoder()\n",
    "\n",
    "# Antes de aplicar el encoding, la data debe ser redimensionada usando \"array.reshape(-1,1)\"\n",
    "data_to_encode = fe_dataset[\"Embarked\"].values.reshape(-1,1)\n",
    "\n",
    "# Se codifica el array anterior y luego se convierte como tal a un array\n",
    "# (ya que la salida se obtiene como una \"sparse matrix\")\n",
    "data_matrix_enc = OHE_embarked.fit_transform(data_to_encode).toarray()\n",
    "\n",
    "# 1. Se convierte la matriz anterior en un dataframe con ints\n",
    "# 2. Se agregan las columnas generadas al dataset\n",
    "fe_dataset[[\"C\", \"Q\", \"S\"]] = pd.DataFrame(data_matrix_enc, columns = [\"C\", \"Q\", \"S\"]).astype(int)\n",
    "\n",
    "# Para análisis posteriores, se crea una columna con label encoding para \"Embarked\"\n",
    "fe_dataset[\"embarked_enc\"] = fe_dataset[\"Embarked\"].astype('category')\n",
    "fe_dataset[\"embarked_enc\"] = fe_dataset[\"embarked_enc\"].cat.codes\n",
    "\n",
    "# =======================\n",
    "# Frequency Encoding \"last_name\" y \"honorific\"\n",
    "# =======================\n",
    "\n",
    "# Columnas a las que se le aplicará frequency encoding\n",
    "freq_enc_columns = [\"last_name\", \"honorific\"]\n",
    "\n",
    "# Se itera sobre cada columna para frequency encoding\n",
    "for column in freq_enc_columns:\n",
    "\n",
    "    # Función que obtiene el porcentaje de ocurrencia de cada categoría\n",
    "    frequency_encode = fe_dataset.groupby(column).size() / len(fe_dataset)\n",
    "\n",
    "    # Se asigna a cada categoría su porcentaje de ocurrencia\n",
    "    fe_dataset[column + \"_enc\"] = fe_dataset[column].apply(lambda x: frequency_encode[x])\n",
    "\n",
    "# =======================\n",
    "# Fare_per_person y group_size\n",
    "# =======================\n",
    "\n",
    "# Se inicializan las nuevas columnas \n",
    "fe_dataset[\"fare_per_person\"] = 0.0             # Se inicializa con floats para obtener decimales \n",
    "fe_dataset[\"group_size\"] = 0                    # Se inicializa con ints para obtener enteros\n",
    "\n",
    "# Se recorre cada fila en el dataset\n",
    "for ind, row in raw_dataset.iterrows():\n",
    "    \n",
    "    # 1. Se extrae el número de ocurrencias para cada tipo de ticket\n",
    "    # 2. Se extrae el tipo de ticket actual y se indexa el resultado anterior\n",
    "    # 3. El resultado es el tamaño de grupo \n",
    "    group_size = raw_dataset[\"Ticket\"].value_counts()[row[\"Ticket\"]]\n",
    "\n",
    "    # Se agrega el precio por persona y tamaño de grupo a cada columna\n",
    "    fe_dataset.at[ind, \"fare_per_person\"] = row[\"Fare\"] / group_size\n",
    "    fe_dataset.at[ind, \"group_size\"] = group_size\n",
    "\n",
    "# Se elimina la columna de fare previa\n",
    "fe_dataset = fe_dataset.drop(\"Fare\", axis = 1)\n",
    "\n",
    "# =======================\n",
    "# Ticket number y prefix\n",
    "# =======================\n",
    "\n",
    "# Se divide la variable de Ticket en un prefijo y un número\n",
    "fe_dataset[[\"temp\", \"ticket_prefix\", \"ticket_number\"]] = fe_dataset[\"Ticket\"].str.extract(\"((?P<prefix>.*) )?(?P<number>\\d*)\", expand = True)\n",
    "\n",
    "# La operación anterior genera una columna adicional que consiste \n",
    "# de una copia de \"ticket_prefix\". Se elimina esta columna\n",
    "fe_dataset = fe_dataset.drop(\"temp\", axis = 1)\n",
    "\n",
    "# También se elimina la columna de \"Ticket\"\n",
    "fe_dataset = fe_dataset.drop(\"Ticket\", axis = 1)\n",
    "\n",
    "# Se sustituye la categoría \"nan\" (sin prefijo) por \"No Prefix\"\n",
    "fe_dataset.loc[fe_dataset[\"ticket_prefix\"].isna(), \"ticket_prefix\"] = \"No Prefix\"\n",
    "\n",
    "# Se convierte en un número la columna de \"ticket_number\"\n",
    "fe_dataset[\"ticket_number_int\"] = pd.to_numeric(fe_dataset[\"ticket_number\"])\n",
    "\n",
    "# Al convertir a número \"ticket_number\" habían algunas casillas sin número\n",
    "# que se convirtieron en NaNs. Se igualan los NaNs a 0.\n",
    "fe_dataset.loc[fe_dataset[\"ticket_number_int\"].isna(), \"ticket_number_int\"] = 0\n",
    "\n",
    "# Se hace label encoding del \"ticket_prefix\"\n",
    "fe_dataset[\"ticket_prefix_enc\"] = fe_dataset[column].astype('category')\n",
    "fe_dataset[\"ticket_prefix_enc\"] = fe_dataset[\"ticket_prefix_enc\"].cat.codes\n",
    "\n",
    "# =======================\n",
    "# Imputación Age\n",
    "# =======================\n",
    "\n",
    "# 1. Se agrupan los datos según la clase \n",
    "# 2. Se rellenan los valores faltantes de todas las columnas con su media\n",
    "# 3. Se guarda únicamente la columna \"rellena\" de edad\n",
    "fe_dataset.loc[:, \"Age\"] = fe_dataset.groupby([\"passenger_class\"]).transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# =======================\n",
    "# Normalización\n",
    "# =======================\n",
    "\n",
    "# Columnas a normlizar\n",
    "columns_to_normalize = [\"SibSp\", \"Parch\", \"Age\", \"fare_per_person\", \"ticket_number_int\"]\n",
    "\n",
    "# Se itera sobre cada columna a normalizar\n",
    "for column in columns_to_normalize:\n",
    "\n",
    "    fe_dataset[column + \"_norm\"] = (fe_dataset[column] - fe_dataset[column].mean()) / fe_dataset[column].std()"
   ]
  },
  {
   "source": [
    "## Selección de Variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se seleccionan las columnas a utilizar\n",
    "# Por el momento se excluye el ticket number porque cuenta con valores nulos\n",
    "proc_dataset = fe_dataset[[\"Age\", \"Age_norm\", \"SibSp\", \"SibSp_norm\", \"Parch\", \"Parch_norm\", \"fare_per_person\", \"fare_per_person_norm\", \n",
    "                           \"C\", \"Q\", \"S\", \"embarked_enc\", \"passenger_class_enc\", \"passenger_sex_enc\", \"Male\", \"Female\", \"last_name_enc\", \n",
    "                           \"honorific_enc\", \"married\", \"ticket_prefix_enc\", \"ticket_number_int\", \"ticket_number_int_norm\", \"passenger_survived_enc\"]]\n",
    "\n",
    "# Se renombran las columnas\n",
    "proc_dataset.columns = [\"age\", \"age_norm\", \"sibsp\", \"sibsp_norm\", \"parch\", \"parch_norm\", \"fare_per_person\", \"fare_per_person_norm\",\n",
    "                        \"C\", \"Q\", \"S\", \"embarked\", \"passenger_class\", \"passenger_sex\", \"male\", \"female\", \"last_name\", \"honorific\", \n",
    "                        \"married\", \"ticket_prefix\", \"ticket_number\", \"ticket_number_norm\", \"survived\"]"
   ]
  },
  {
   "source": [
    "## Declaración de Funciones\n",
    "\n",
    "Función para predecir con Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictNBC(data, model):\n",
    "\n",
    "    # Se extraen los elementos de la tupla de \"model\"\n",
    "    P_priori = model[0]\n",
    "    dists = model[1]\n",
    "\n",
    "    # Lista vacía de predicciones\n",
    "    preds = []\n",
    "\n",
    "    # Se inicializa la cuenta de predicciones correctas en 0\n",
    "    correct = 0\n",
    "\n",
    "    # Por cada fila en el dataframe \"data\"\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "        # Se inicializan las probabilidades por label\n",
    "        P = dict()\n",
    "\n",
    "        # Se extraen las features (todas las columnas menos la última) y el target\n",
    "        x_pred = row.iloc[0:len(data.columns)-1]\n",
    "        y_pred = row[\"survived\"]\n",
    "\n",
    "        # Por cada label en \"survived\"\n",
    "        for label in P_priori.keys():\n",
    "\n",
    "            # El valor inicial de la probabilidad de la label actual\n",
    "            # es igual al valor de P(y) (probabilidad a priori)\n",
    "            P[label] = P_priori[label]\n",
    "\n",
    "            # Por cada columna en las features\n",
    "            for col, val in x_pred.iteritems():\n",
    "\n",
    "                # Si es una variable categórica\n",
    "                if is_integer_dtype(data[col]):\n",
    "\n",
    "                    # P(y|x1,x2,...,xn) = P(x1|y) * P(x2|y) * ...* P(xn|y) * P(y)\n",
    "                    P[label] *= dists[label][col][val]\n",
    "\n",
    "                # Si es una variable numérica\n",
    "                elif is_float_dtype(data[col]):\n",
    "                    \n",
    "                    # P(y|x1,x2,...,xn) = P(x1|y) * P(x2|y) * ...* P(xn|y) * P(y)\n",
    "                    P[label] *= dists[label][col](val)\n",
    "\n",
    "\n",
    "        # Se realiza la predicción:\n",
    "        # Se obtiene la label del valor con la mayor probabilidad\n",
    "        pred = max(P, key = P.get)\n",
    "        preds.append(pred) \n",
    "\n",
    "        # Se suma 1 a la cuenta de correctos si la predicción es igual al valor real\n",
    "        if pred == y_pred:\n",
    "            correct += 1\n",
    "\n",
    "    # Se calcula el accuracy\n",
    "    accuracy = correct / len(data)\n",
    "\n",
    "    return preds, accuracy      "
   ]
  },
  {
   "source": [
    "Función para predecir con Regresión Logística"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(df, theta):\n",
    "\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "    # =======================================\n",
    "    # CREACIÓN DE GRAFO\n",
    "    # =======================================\n",
    "\n",
    "    # Dimensiones del modelo\n",
    "    NumCols = theta.shape[0]\n",
    "    NumCat = theta.shape[1]\n",
    "\n",
    "    # Se extraen los datos \"x\" y \"y\" del dataframe\n",
    "    x_test = df.iloc[:, 0:len(df.columns)-1]\n",
    "    y_test = df[\"survived\"]\n",
    "\n",
    "    # Se convierten los dataframes en arrays de numpy\n",
    "    x_test = x_test.to_numpy()\n",
    "    y_test = y_test.to_numpy()\n",
    "\n",
    "    # Se redimensiona el vector de labels\n",
    "    y_test = np.reshape(y_test, (-1, 1))\n",
    "\n",
    "    # Se le agrega una fila de unos a \"train_x\" convirtiendo los datos en un polinomio de grado 1\n",
    "    poly = PolynomialFeatures(1)\n",
    "    x_test = poly.fit_transform(x_test)\n",
    "\n",
    "    # Se reinicia la creación del grafo creado\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Se crea un objeto de tipo grafo\n",
    "    grafo = tf.Graph()\n",
    "\n",
    "    # Se incluyen nodos dentro del grafo\n",
    "    with grafo.as_default():\n",
    "\n",
    "        # Input: Se definen los datos de entrenamiento\n",
    "        # X_test: Tantas filas como imágenes (None para un tamaño variable). Tantas columnas como pixeles (+ 1 columna de unos)\n",
    "        # Y_test: Tantas filas como imágenes (None para un tamaño variable). 1 columna\n",
    "        X = tf.placeholder(tf.float32, [None, NumCols], \"X\")\n",
    "        Y = tf.placeholder(tf.float32, [None, 1], \"Y\")\n",
    "\n",
    "        # Input: Parámetros de regresión lineal\n",
    "        # Tantas filas como columnas tiene X_test. Tantas columnas como categorías de regresión hay\n",
    "        params = tf.placeholder(dtype=\"float\", shape=[NumCols, NumCat], name=\"params\")\n",
    "\n",
    "        # Predicción de la salida dados los parámetros \n",
    "        # Dims Output: (NoMuestras, NoPixeles) x (NoPixeles, NoCategorias) = (NoMuestras, NoCategorias)\n",
    "        with tf.name_scope(\"Predict\"):\n",
    "            \n",
    "            # Se calculan los logits (m*X + B = Logits)\n",
    "            logits = tf.matmul(X, params)\n",
    "\n",
    "            # Se calcula manualmente el valor de la estimación usando sigmoid\n",
    "            # (usado para el cálculo del accuracy)\n",
    "            probs = tf.nn.sigmoid(logits)\n",
    "\n",
    "            # Se convierten las probabilidades anteriores en una predicción\n",
    "            Y_hat = tf.round(probs)\n",
    "\n",
    "        # Cálculo del error por medio de entropía cruzada\n",
    "        with tf.name_scope(\"Cross_Entropy\"):\n",
    "            \n",
    "            # Cálculo de la entropía cruzada\n",
    "            error = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "\n",
    "            # tf.nn.softmax_cross_entropy_with_logits retorna el \"loss\" de cada fila o muestra\n",
    "            # Se promedian todas las filas para obtener el costo final\n",
    "            error = tf.reduce_mean(error)\n",
    "\n",
    "            # Incluir el error en la parte de \"Scalars\" de Tensorboard\n",
    "            error_summary = tf.summary.scalar(\"Error\", error)\n",
    "\n",
    "        with tf.name_scope(\"Accuracy\"):\n",
    "\n",
    "            # Se chequea cuales predicciones son iguales a las labels reales (element-wise)\n",
    "            correct_pred = tf.math.equal(Y_hat, Y)\n",
    "\n",
    "            # Se suma el número de predicciones correctas y luego se divide dentro del número total de predicciones\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "            \n",
    "            # Se incluye el accuracy en la parte de \"Scalars\" de Tensorboard\n",
    "            accuracy_summary = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "        # Inicializar variables globales\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    # =======================================\n",
    "    # EJECUCIÓN DE GRAFO\n",
    "    # =======================================\n",
    "    \n",
    "    with tf.Session(graph = grafo) as sess:\n",
    "        \n",
    "        # Inicializa todas las variables de ser necesario\n",
    "        tf.initialize_all_variables().run()\n",
    "\n",
    "        # Inicializar el grafo\n",
    "        sess.run(init)\n",
    "\n",
    "        # Se definen los inputs del grafo\n",
    "        inputs_grafo = {\n",
    "            X : x_test,\n",
    "            Y : y_test,\n",
    "            params: Theta\n",
    "        }\n",
    "\n",
    "        # Se extraen los parámetros resultantes de la regresión y el error\n",
    "        pred = Y_hat.eval(feed_dict=inputs_grafo)\n",
    "        err = error.eval(feed_dict=inputs_grafo)\n",
    "        acc = accuracy.eval(feed_dict=inputs_grafo)\n",
    "\n",
    "    return pred, err, acc"
   ]
  },
  {
   "source": [
    "## Carga de Modelos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import dill as pickle\n",
    "\n",
    "# Carga de los modelos de SVM y decision tree\n",
    "DT_model = joblib.load('./Models/DT_randomstate=102,max_depth=1.pkl')\n",
    "SVM_model = joblib.load('./Models/SVM_C=3,degree=2,gamma=0.1,coef0=0.pkl')\n",
    "\n",
    "# Carga del modelo de naive bayes\n",
    "with open('./Models/NB.pkl', 'rb') as f:\n",
    "    NB_model = pickle.load(f)\n",
    "\n",
    "# Carga del modelo de regresión logística\n",
    "Theta = np.load(\"./Models/RL_lr=0.01,epochs=6000,batch_size=128.npy\", allow_pickle = True)"
   ]
  },
  {
   "source": [
    "## Ensemble Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ensemble Accuracy Score: 0.8206\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Se vuelve a shufflear el processed dataset\n",
    "proc_dataset.sample(frac = 1).reset_index(drop = True)\n",
    "\n",
    "# Se obtiene una muestra con el 60% de las filas\n",
    "ensemble_dataset = proc_dataset.sample(frac=0.6)\n",
    "\n",
    "# 1. Se obtiene la \"ground truth\" (columna de survived)\n",
    "# 2. Se convierte en un array de numpy\n",
    "# 3. Se le hace un reshape para convertirlo en un vector columna\n",
    "ground_truth = ensemble_dataset[\"survived\"].to_numpy()\n",
    "ground_truth = np.reshape(ground_truth, (-1,1))\n",
    "\n",
    "# ===============================\n",
    "# Decision Tree\n",
    "# ===============================\n",
    "\n",
    "# Se extraen las variables de interés\n",
    "DT_data = ensemble_dataset[[\"age\", \"sibsp\", \"parch\", \"fare_per_person\", \"C\", \"Q\", \"S\", \"passenger_class\", \"passenger_sex\", \"last_name\", \"honorific\", \"ticket_prefix\",\n",
    "                            \"ticket_number\", \"married\", \"survived\"]]\n",
    "\n",
    "# Se separan las features y la variable objetivo\n",
    "DT_x = DT_data.iloc[:, 0:len(DT_data.columns)-1]\n",
    "\n",
    "# Se generan las predicciones con el modelo\n",
    "# Tipo de output: Array (-1,)\n",
    "DT_preds = DT_model.predict(DT_x)\n",
    "\n",
    "# Reshape para convertirlo en vector columna\n",
    "DT_preds = np.reshape(DT_preds, (-1,1))\n",
    "\n",
    "# ===============================\n",
    "# SVM\n",
    "# ===============================\n",
    "\n",
    "# Se extraen las variables de interés\n",
    "SVM_data = ensemble_dataset[[\"age_norm\", \"sibsp_norm\", \"parch_norm\", \"fare_per_person_norm\", \"C\", \"Q\", \"S\", \"passenger_class\", \"passenger_sex\", \"last_name\", \"honorific\", \"ticket_prefix\",\n",
    "                              \"ticket_number_norm\", \"married\", \"survived\"]]\n",
    "\n",
    "# Se separan las features y la variable objetivo\n",
    "SVM_x = SVM_data.iloc[:, 0:len(SVM_data.columns)-1]\n",
    "\n",
    "# Se generan las predicciones con el modelo\n",
    "# Tipo de output: Array (-1,)\n",
    "SVM_preds = SVM_model.predict(SVM_x)\n",
    "\n",
    "# Reshape para convertirlo en vector columna\n",
    "SVM_preds = np.reshape(SVM_preds, (-1,1))\n",
    "\n",
    "# ===============================\n",
    "# Naive Bayes\n",
    "# ===============================\n",
    "\n",
    "# Se extraen las variables de interés (mismas variables que para el SVM)\n",
    "NB_data = SVM_data\n",
    "\n",
    "# Se hace la predicción y se evalúa la precisión\n",
    "# Tipo de output: Lista\n",
    "NB_preds, NB_accuracy = PredictNBC(NB_data, NB_model)\n",
    "\n",
    "# Se convierte en array y luego se le hace reshape\n",
    "# para convertirlo en vector columna\n",
    "NB_preds = np.reshape(np.array(NB_preds), (-1,1))\n",
    "\n",
    "# ===============================\n",
    "# Regresión Logística\n",
    "# ===============================\n",
    "\n",
    "# Se extraen las variables de interés (mismas variables que para el SVM)\n",
    "RL_data = SVM_data\n",
    "\n",
    "# Se hace la predicción y se evalúa la precisión\n",
    "# Tipo de output: Array (-1, 1)\n",
    "RL_preds, RL_error, RL_accuracy_test = Predict(RL_data, Theta)\n",
    "\n",
    "# ===============================\n",
    "# Votación Mayoritaria\n",
    "# ===============================\n",
    "\n",
    "# Se concatenan horizontalmente los resultados\n",
    "Results = np.hstack((DT_preds, SVM_preds, NB_preds, RL_preds))\n",
    "\n",
    "# Se obtiene la moda de cada fila (voto mayoritario)\n",
    "Vote = stats.mode(Results, axis = 1)[0]\n",
    "\n",
    "# Se evalúa la precisión\n",
    "# 1. Se obtiene el número de valores iguales entre la predicción y la salida real\n",
    "# 2. Se divide este número dentro del número de muestras del dataset de validación\n",
    "E_accuracy = list(Vote == ground_truth).count(True) / len(ground_truth)\n",
    "\n",
    "# Se imprime la precisión de ensemble\n",
    "print(\"Ensemble Accuracy Score:\", \"%.4f\" % round(E_accuracy, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}