{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Ensayo SVM\n",
    "\n",
    "Debe ser agregado al proyecto en un notebook con descripción, texto, imágenes, etc en markdown y ejemplos en código:\n",
    "\n",
    "- hipótesis de SVM\n",
    "- función de costo\n",
    "- Algoritmo de aprendizaje/entrenamiento\n",
    "- Propiedades, similitudes, diferencias ,ventajas y desventajas sobre otros algoritmos y modelos.\n",
    "- Kernel-trick/basis functions\n",
    "\n",
    "Referencias útiles:\n",
    "- [Contenido SVM galileo(2020)](https://youtu.be/pdR9_eylib8?t=6134)\n",
    "- [Hands on Machine Learning with sklearn and tensorflow (capitulo 5)](https://github.com/yanshengjia/ml-road/blob/master/resources/Hands%20On%20Machine%20Learning%20with%20Scikit%20Learn%20and%20TensorFlow.pdf)\n",
    "- [Support vector machines(Stanford CS229 Machine Lerning 2018)](https://youtu.be/lDwow4aOrtg)\n",
    "- [Kernel trick](https://www.youtube.com/watch?v=vMmG_7JcfIc)\n",
    "\n",
    "Videos de estudiantes de años previos:\n",
    "- https://youtu.be/VWwb3IAB6Rc\n",
    "- https://youtu.be/envRLLZmio8\n",
    "\n",
    "\n",
    "------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Principios\n",
    "\n",
    "La idea básica de un SVM (support vector machine) es la de crear una estructura similar a la de una red neuronal de pocas capas: Consiste en crear un hiperplano \"óptimo\" capaz de separar datos separables por fronteras lineales. Aquellos patrones que no son separables por fronteras lineales, entonces se pueden transformar a nuevos espacios dimensionales donde su separación sea mucho más sencilla. A la función empleada para realizar esta transformación se le denomina \"función kernel\". Pero, ¿qué es un support vector?\n",
    "\n",
    "Los support vectors consisten de los puntos de datos que están más cercanos a la frontera de decisión, en otras palabras, son los datos más difíciles de clasificar en un problema de esta índole debido a que son los más propensos a ser mal clasificados. Tomando esto en cuenta, se puede llegar a establecer que el hiperplano óptimo de separación es aquel que proviene de una función con el menor número posible de parámetros o variables con las que se debe de jugar para poder obtener una buena separación entre clases. \n",
    "\n",
    "En este sentido, un SVM maximiza el margen (la distancia del hiperplano a los support vectors) alrededor del hiperplano. Además, se puede probar que este es óptimo ya que este únicamente depende de un número pequeño de variable, en este caso, cada uno de los support vectors. \n",
    "\n",
    "![svm1](Media/svm1.PNG)\n",
    "\n",
    "\n",
    "## Hipótesis de SVM\n",
    "\n",
    "La hipótesis de un SVM establece lo siguiente\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x)=\\left\\{\\begin{array}{ll}\n",
    "1 & \\text { if } \\theta^{T} x>=0 \\\\\n",
    "0 & \\text { otherwise }\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Debido a esto es que se observa que la función de costo tiene diferentes comportamientos en diferentes secciones. Se trata de una regresión lineal simple en una parte y luego un valor constante en el resto de su dominio. Debido a esto, la forma de la función de costo de una SVM y una regresión logística son muy parecidas.\n",
    "\n",
    "![svm1](Media/svm2.PNG)\n",
    "\n",
    "## Función de Costo\n",
    "\n",
    "Para una función frontera lineal, la función de costo para un SVM es la siguiente\n",
    "\n",
    "$$\n",
    "\\operatorname{Cost}\\left(h_{\\theta}(x), y\\right)=\\left\\{\\begin{array}{ll}\n",
    "\\max \\left(0,1-\\theta^{T} x\\right) & \\text { if } y=1 \\\\\n",
    "\\max \\left(0,1+\\theta^{T} x\\right) & \\text { if } y=0\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Esto implica que la derivada o la \"loss function\" de un SVM es la siguiente:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "J(\\theta)=\\sum_{i=1}^{m} y^{(i)} \\operatorname{Cost}_{1}\\left(\\theta^{T}\\left(x^{(i)}\\right)+\\left(1-y^{(i)}\\right) \\operatorname{Cost}_{0}\\left(\\theta^{T}\\left(x^{(i)}\\right)\\right.\\right. \\\\\n",
    "J(\\theta)=\\sum_{i=1}^{m} y^{(i)} \\max \\left(0,1-\\theta^{T} x\\right)+\\left(1-y^{(i)}\\right) \\max \\left(0,1+\\theta^{T} x\\right)\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "Si se desea, también se puede adicionar regularización a un SVM. Por ejemplo, al adicional regularización L2, la función de $J(\\theta)$ cambia a:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "J(\\theta)=C\\left[\\sum _ { i = 1 } ^ { m } y ^ { ( i ) } \\operatorname { C o s t } _ { 1 } \\left(\\theta^{T}\\left(x^{(i)}\\right)+\\left(1-y^{(i)}\\right) \\operatorname{Cost}_{0}\\left(\\theta^{T}\\left(x^{(i)}\\right)\\right]+\\frac{1}{2} \\sum_{j=1}^{n} \\theta_{j}^{2}\\right.\\right. \\\\\n",
    "m=\\text { number of samples, } \\quad n=\\text { number of features }\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "En caso el tipo de SVM sea no lineal, la función de costo se mantiene virtualmente idéntica, con la única diferencia que la variable $x$ es sustituida por $f$. \n",
    "\n",
    "$$\n",
    "J(\\theta)=C\\left[\\sum _ { i = 1 } ^ { m } y ^ { ( i ) } \\operatorname { C o s t } _ { 1 } \\left(\\theta^{T}\\left(f^{(i)}\\right)+\\left(1-y^{(i)}\\right) \\operatorname{Cost}_{0}\\left(\\theta^{T}\\left(f^{(i)}\\right)\\right]\\right.\\right.\n",
    "$$\n",
    "\n",
    "En este caso $f$ consiste de una función dependiente de X a la cual le denominamos \"función kernel\".\n",
    "\n",
    "## Kernels\n",
    "\n",
    "Imaginémonos que contamos con una muestra $x$ que cuenta con dos dimensiones $x_1$ y $x_2$. Luego colocamos puntos al azar sobre el espacio dimensional y les denominamos \"landmarks\" ($l^1$, $l^2$, $l^3$). Luego, queremos conocer que tan cerca está nuestra muestra $x$ a las landmarks. Para conocer esto utilizamos una función de similitud, la cual comúnmente denotamos de la siguiente forma: $k(x, l^n)$. Estas funciones \"k\" son las denominadas \"kernels\" y nos permiten medir esta \"proximidad\" nuestras landmarks a través de diferentes medidas de distancia. El kernel gaussiano por ejemplo, utiliza la distancia euclideana para calcular la distancia. \n",
    "\n",
    "Estas transformaciones son súmamente útiles cuando los datos no son linearmente separables. En estos casos, la aplicación de la transformación permite convertir el espacio dimensional actual en un nuevo espacio donde la features ahora son fácilmente separables.\n",
    "\n",
    "![svm1](Media/svm3.PNG)\n",
    "\n",
    "La idea por lo tanto, es ganar \"separación lineal\" mapeando la data a un espacio de mayor dimensionalidad, en su mayor parte, empleando funciones de transformación no lineales. Algunas de las más conocidas se presentan a continuación\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "K(\\mathbf{x}, \\mathbf{y})=(\\mathbf{x} \\cdot \\mathbf{y}+1)^{p} \\\\\n",
    "K(\\mathbf{x}, \\mathbf{y})=\\exp \\left\\{-\\|\\mathbf{x}-\\mathbf{y}\\|^{2} / 2 \\sigma^{2}\\right\\rceil \\\\\n",
    "K(\\mathbf{x}, \\mathbf{y})=\\tanh (\\kappa \\mathbf{x} \\cdot \\mathbf{y}-\\delta)\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "La primera consiste de una transformación polinomial, la segunda consiste de una transformación gaussiana radial (radial basis function) y finalmente la tercera es un sigmoide. La segunda por ejemplo, facilita la separación de datos como los presentados a continuación\n",
    "\n",
    "![svm1](Media/svm4.PNG)\n",
    "\n",
    "Estos no podrían ser separables por un hiperplano tradicional, pero al convertirlo utilizando la ecuación para expresar seno y coseno con exponenciales, la división se vuelve mucho más sencilla. De aquí proviene el hecho de aque a esta transformación comúnmente se le llame \"kernel-trick\".\n",
    "\n",
    "## Algoritmo de Aprendizaje Entrenamiento\n",
    "\n",
    "El entrenamiento de un SVM se divide en diferentes partes. Como se mencionó previamente, un SVM es muy similar a una regresión lineal, por lo que este también cuenta con learning rate, epochs, iteraciones, batch size y una matriz de parámetros W. Por lo tanto el algoritmo de entrenamiento consiste en:\n",
    "\n",
    "- Inicializar el valor de W\n",
    "- Calcular el valor del error \n",
    "- Determinar el valor de los gradientes para cada miembro de W utilizando descenso de gradiente\n",
    "- Actualizar el valor de W utilizando el learning rate y los gradientes\n",
    "- Repetir hasta finalizar las iteraciones.\n",
    "\n",
    "De nuevo, este es muy similar al de una regresión lineal. La diferencia es la utilización de los support vectors y de los kernels\n",
    "\n",
    "## Propiedades\n",
    "\n",
    "- Debido a su similitud con la regresión logística, los SVMs cuentan con una función de costo convexa, la cual se puede solucionar encontrando la respuesta a un problema cuadrático. El método más popular para poder obtener dicha solución es la utilización de optimización secuencial mínima, la cual puede separar problemas cuadráticos de gran dimensionalidad, en problemas cuadráticos de mucha menor escala que pueden ser resueltos de forma analítica mucho más fácil.\n",
    "\n",
    "- Para obtener buenos resultados con este modelo, y prevenir overfitting, se debe elegir un buen valor para el término de regularización L2: $C$. Además, también se puede ajustar el valor de $\\sigma^2$ para mejorar el balance entre bias y varianza. \n",
    "\n",
    "- Cuando se tiene un alto número de \"features\", un SVM linear podría llegar a ser útil. Si se cuenta con un número pequeño de features (menos de 100) y un número no muy elevado de features, entonces SVM con un kernel gaussiano podría ser la solución.\n",
    "\n",
    "- Si el valor de regularización no se ajusta debidamente, el SVM sufrirá de overfit casi seguramente, ya que el SVM comenzará a tomar cada punto como un support vector. \n",
    "\n",
    "## Ventajas\n",
    "\n",
    "- Los SVMs pueden ayudar al investigador a generar un modelo rápido para datos de los cuales se desconoce la forma o no se ha realizado ningún análisis riguroso. \n",
    "- Funciona bien incluso con data no estructurada como texto, imágenes o árboles.\n",
    "- No es susceptible a mínimos locales\n",
    "- En la práctica, todo SVM cuenta con un poco de generalización, por lo que es más difícil de hacerle sufrir de overfitting\n",
    "- Escala bien con datasets de alta dimensionalidad\n",
    "\n",
    "\n",
    "## Desventajas\n",
    "\n",
    "- La elección de un kernel adecuado es una tarea laboriosa\n",
    "- Para datasets grandes el tiempo de entrenamiento comienza a incrementar drásticamente\n",
    "- La legibilidad del modelo final es difícil de interpretar de forma intuitiva\n",
    "- Es dificil de \"tunear\" los parámetros C y gamma del modelo. Su efecto no es fácilmente visualizable\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- [R. Berwick](https://web.mit.edu/6.034/wwwbob/svm.pdf)\n",
    "- [FreeCodeCamp](https://www.freecodecamp.org/news/support-vector-machines/)\n",
    "- [Shuyu Luo](https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-iii-5dff33fa015d)\n",
    "- [Hole House](https://www.holehouse.org/mlclass/12_Support_Vector_Machines.html)\n",
    "- [Statinfer](https://statinfer.com/204-6-8-svm-advantages-disadvantages-applications/)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}